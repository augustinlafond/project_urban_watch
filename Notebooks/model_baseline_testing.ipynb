{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e4e6d25",
   "metadata": {},
   "source": [
    "# Notebook - v3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb030d9",
   "metadata": {},
   "source": [
    "Notebook that incorporates Kevin work on preprocessing data and for testing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1930fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774915e0",
   "metadata": {},
   "source": [
    "### Model baseline testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "955691a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/clairedebadts/.pyenv/versions/urban_watch/lib/python3.10/site-packages/google/api_core/_python_version_support.py:266: FutureWarning: You are using a Python version (3.10.6) which Google will stop supporting in new releases of google.api_core once it reaches its end of life (2026-10-04). Please upgrade to the latest Python version, or at least Python 3.11, to continue receiving updates for google.api_core past that date.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sentinelhub import SentinelHubRequest, DataCollection, MimeType, CRS, BBox, SHConfig\n",
    "from dotenv import load_dotenv\n",
    "from pyproj import Transformer\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import rasterio\n",
    "import folium\n",
    "import cv2\n",
    "from urban_watch.ml_logic.data import load_data\n",
    "from s2cloudless import S2PixelCloudDetector\n",
    "from urban_watch.ml_logic.package import CloudMasker\n",
    "from urban_watch.ml_logic.package import preprocess_image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from urban_watch.ml_logic.data import load_data\n",
    "from urban_watch.ml_logic.labels import (\n",
    "    get_bbox_from_features,\n",
    "    bbox_to_wgs84,\n",
    "    tile_name_from_bbox_wgs84,\n",
    "    get_label_array\n",
    ")\n",
    "from urban_watch.ml_logic.package import CloudMasker, DataCleaner, IndexCalculator\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8800de07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentinelhub import SHConfig\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f8f0e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()  # charge automatiquement le .env du dossier courant\n",
    "\n",
    "config = SHConfig()\n",
    "config.sh_client_id = os.environ.get(\"SH_CLIENT_ID\")\n",
    "config.sh_client_secret = os.environ.get(\"SH_CLIENT_SECRET\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3005971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "urbanwatch-labels\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getenv(\"BUCKET_NAME\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e209d059",
   "metadata": {},
   "source": [
    "Cellule 1 — Imports + chargement des X & Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b74c1bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ X loaded: (10, 300, 300, 10)\n",
      "✔ Y loaded: (10, 300, 300)\n"
     ]
    }
   ],
   "source": [
    "# 1 Charger les features X (tuiles Sentinel)\n",
    "X_array, meta = load_data()\n",
    "print(\"✔ X loaded:\", X_array.shape)  # (n_tiles, 300, 300, 10)\n",
    "\n",
    "#  Charger les bbox pour aller chercher les Y\n",
    "bbox_x, crs_x = get_bbox_from_features()\n",
    "bbox_wgs = bbox_to_wgs84(bbox_x, crs_x)\n",
    "tile_names = tile_name_from_bbox_wgs84(bbox_wgs)\n",
    "\n",
    "# 3) Charger les labels WorldCover (tuiles Y 300x300)\n",
    "Y_tiles = get_label_array(tile_names, bbox_wgs, bbox_x, crs_x)\n",
    "Y_array = np.stack(Y_tiles, axis=0)  # (n_tiles, 300, 300)\n",
    "\n",
    "print(\"✔ Y loaded:\", Y_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee09a615",
   "metadata": {},
   "source": [
    "Cellule 2 — Conversion des labels en binaire (urban / non urban)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd472eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_binary shape: (10, 300, 300)\n",
      "Unique values: (array([0, 1]), array([529317, 370683]))\n"
     ]
    }
   ],
   "source": [
    "def worldcover_to_binary(tile, urban_classes={50}):\n",
    "    \"\"\"\n",
    "    tile : array (H, W) avec codes ESA WorldCover\n",
    "    urban_classes : set des codes considérés comme urban\n",
    "\n",
    "    Retourne : array (H, W) binaire {0,1}\n",
    "    \"\"\"\n",
    "    mask_urban = np.isin(tile, list(urban_classes))\n",
    "    return mask_urban.astype(int)\n",
    "\n",
    "Y_binary = np.stack([worldcover_to_binary(t) for t in Y_array], axis=0)\n",
    "\n",
    "print(\"Y_binary shape:\", Y_binary.shape)\n",
    "print(\"Unique values:\", np.unique(Y_binary, return_counts=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bbf7a7",
   "metadata": {},
   "source": [
    "Cellule 3 — Préprocessing par tuile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7f3cc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image_for_baseline(img):\n",
    "    \"\"\"\n",
    "    img : (H, W, 10) brut Sentinel-2\n",
    "\n",
    "    Retourne :\n",
    "      - img_std : (H, W, 13)  -> 10 bandes normalisées + 3 indices (ndvi, ndbi, mndwi)\n",
    "      - mask_valid : (H, W) bool -> pixels utilisables (pas NaN)\n",
    "    \"\"\"\n",
    "    cleaner = DataCleaner()\n",
    "    cloud_detector = CloudMasker()\n",
    "\n",
    "    # 1) Normalisation des bandes (0-1)\n",
    "    img_norm = cleaner.normalize_bands(img)\n",
    "\n",
    "    # 2) Détection nuages\n",
    "    cloud_mask = cloud_detector.detect_clouds(img_norm)\n",
    "\n",
    "    # 3) Masquage nuages (nan)\n",
    "    img_masked = cloud_detector.apply_mask(img_norm, cloud_mask, fill_value=np.nan)\n",
    "\n",
    "    # 4) Indices spectraux\n",
    "    B3  = img_masked[:, :, CloudMasker.BAND_IDX[\"B03\"]]\n",
    "    B4  = img_masked[:, :, CloudMasker.BAND_IDX[\"B04\"]]\n",
    "    B8  = img_masked[:, :, CloudMasker.BAND_IDX[\"B08\"]]\n",
    "    B11 = img_masked[:, :, CloudMasker.BAND_IDX[\"B11\"]]\n",
    "\n",
    "    ndvi  = IndexCalculator.ndvi(B4,  B8)\n",
    "    ndbi  = IndexCalculator.ndbi(B11, B8)\n",
    "    mndwi = IndexCalculator.mndwi(B3,  B11)\n",
    "\n",
    "    ndvi  = ndvi[..., np.newaxis]\n",
    "    ndbi  = ndbi[..., np.newaxis]\n",
    "    mndwi = mndwi[..., np.newaxis]\n",
    "\n",
    "    # 5) Concaténer 10 bandes + 3 indices = 13 canaux\n",
    "    img_13 = np.concatenate([img_masked, ndvi, ndbi, mndwi], axis=-1)\n",
    "\n",
    "    # 6) Standardisation (0 mean / 1 std par bande)\n",
    "    img_std = cleaner.standardize(img_13)\n",
    "\n",
    "    # 7) Mask valid (aucun NaN sur les 13 canaux)\n",
    "    mask_valid = ~np.isnan(img_std).any(axis=-1)\n",
    "\n",
    "    return img_std, mask_valid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74b842c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_processed: (10, 300, 300, 13)\n",
      "valid_masks: (10, 300, 300)\n"
     ]
    }
   ],
   "source": [
    "# Appliquer à toutes les tuiles\n",
    "X_processed = []\n",
    "valid_masks = []\n",
    "\n",
    "for img in X_array:\n",
    "    img_std, mask_valid = preprocess_image_for_baseline(img)\n",
    "    X_processed.append(img_std)\n",
    "    valid_masks.append(mask_valid)\n",
    "\n",
    "X_processed = np.stack(X_processed, axis=0)   # (n_tiles, H, W, 13)\n",
    "valid_masks = np.stack(valid_masks, axis=0)   # (n_tiles, H, W)\n",
    "\n",
    "print(\"X_processed:\", X_processed.shape)\n",
    "print(\"valid_masks:\", valid_masks.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972e7f34",
   "metadata": {},
   "source": [
    "Cellule 4 — Construction du dataset pixel-wise (X_pixels, y_pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28d2b26a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_pixels: (449451, 13)\n",
      "y_pixels: (449451,)\n",
      "Classe distribution: (array([0, 1]), array([300964, 148487]))\n"
     ]
    }
   ],
   "source": [
    "X_pixels_list = []\n",
    "y_pixels_list = []\n",
    "\n",
    "n_tiles, H, W, C = X_processed.shape\n",
    "\n",
    "for i in range(n_tiles):\n",
    "    img13 = X_processed[i]      # (H, W, 13)\n",
    "    mask  = valid_masks[i]      # (H, W)\n",
    "    y_tile = Y_binary[i]        # (H, W)\n",
    "\n",
    "    # On ne garde que les pixels valides\n",
    "    valid = mask\n",
    "\n",
    "    X_pix = img13[valid]        # (n_valid_pixels, 13)\n",
    "    y_pix = y_tile[valid]       # (n_valid_pixels,)\n",
    "\n",
    "    X_pixels_list.append(X_pix)\n",
    "    y_pixels_list.append(y_pix)\n",
    "\n",
    "X_pixels = np.concatenate(X_pixels_list, axis=0)\n",
    "y_pixels = np.concatenate(y_pixels_list, axis=0)\n",
    "\n",
    "print(\"X_pixels:\", X_pixels.shape)\n",
    "print(\"y_pixels:\", y_pixels.shape)\n",
    "print(\"Classe distribution:\", np.unique(y_pixels, return_counts=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be7de97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN in X_pixels: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"NaN in X_pixels:\", np.isnan(X_pixels).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "97e6d687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN in y_pixels: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"NaN in y_pixels:\", np.isnan(y_pixels).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368c2e36",
   "metadata": {},
   "source": [
    "Cellule 5 — Train / Test split pixel-wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b33b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 359560\n",
      "Test size: 89891\n",
      "Train distribution: (array([0, 1]), array([240771, 118789]))\n",
      "Test distribution: (array([0, 1]), array([60193, 29698]))\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_pixels, y_pixels,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_pixels \n",
    ")\n",
    "\n",
    "print(\"Train size:\", X_train.shape[0])\n",
    "print(\"Test size:\", X_test.shape[0])\n",
    "print(\"Train distribution:\", np.unique(y_train, return_counts=True))\n",
    "print(\"Test distribution:\", np.unique(y_test, return_counts=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01abda08",
   "metadata": {},
   "source": [
    "Cellule 6 — Modèles baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0c22079d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogReg accuracy: 0.6920270104904829\n",
      "LogReg f1-score: 0.6021384840907131\n",
      "\n",
      "Classification report (LogReg):\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.69      0.75     60193\n",
      "           1       0.53      0.71      0.60     29698\n",
      "\n",
      "    accuracy                           0.69     89891\n",
      "   macro avg       0.68      0.70      0.68     89891\n",
      "weighted avg       0.73      0.69      0.70     89891\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Logistic regression\n",
    "\n",
    "log_reg = LogisticRegression(max_iter=500, class_weight=\"balanced\")\n",
    "\n",
    "log_reg.fit(X_train, y_train)\n",
    "y_pred_lr = log_reg.predict(X_test)\n",
    "\n",
    "print(\"LogReg accuracy:\", accuracy_score(y_test, y_pred_lr))\n",
    "print(\"LogReg f1-score:\", f1_score(y_test, y_pred_lr))\n",
    "print(\"\\nClassification report (LogReg):\\n\")\n",
    "print(classification_report(y_test, y_pred_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a1b7c189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF accuracy: 0.8656706455596223\n",
      "RF f1-score: 0.7850237675586177\n",
      "\n",
      "Classification report (RandomForest):\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.93      0.90     60193\n",
      "           1       0.83      0.74      0.79     29698\n",
      "\n",
      "    accuracy                           0.87     89891\n",
      "   macro avg       0.86      0.83      0.84     89891\n",
      "weighted avg       0.86      0.87      0.86     89891\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=None,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "print(\"RF accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
    "print(\"RF f1-score:\", f1_score(y_test, y_pred_rf))\n",
    "print(\"\\nClassification report (RandomForest):\\n\")\n",
    "print(classification_report(y_test, y_pred_rf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f53e709a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HGB accuracy: 0.8138968306059561\n",
      "HGB f1-score: 0.6944307450636564\n",
      "\n",
      "Classification report (HGB):\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.90      0.87     60193\n",
      "           1       0.76      0.64      0.69     29698\n",
      "\n",
      "    accuracy                           0.81     89891\n",
      "   macro avg       0.80      0.77      0.78     89891\n",
      "weighted avg       0.81      0.81      0.81     89891\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# History Gradient Boosting Classifier\n",
    "\n",
    "hgb = HistGradientBoostingClassifier(\n",
    "    max_depth=8,\n",
    "    learning_rate=0.1,\n",
    "    l2_regularization=0.1,\n",
    "    max_bins=255\n",
    ")\n",
    "\n",
    "hgb.fit(X_train, y_train)\n",
    "y_pred_hgb = hgb.predict(X_test)\n",
    "\n",
    "print(\"HGB accuracy:\", accuracy_score(y_test, y_pred_hgb))\n",
    "print(\"HGB f1-score:\", f1_score(y_test, y_pred_hgb))\n",
    "print(\"\\nClassification report (HGB):\\n\")\n",
    "print(classification_report(y_test, y_pred_hgb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "062b5829",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "model.py — Baseline Random Forest model for UrbanWatch\n",
    "-------------------------------------------------------\n",
    "\n",
    "Compatible with:\n",
    "- data.py (loading X tiles)\n",
    "- labels.py (loading Y tiles)\n",
    "- package.py (preprocess_image => 13 standardized bands)\n",
    "\n",
    "Pipeline:\n",
    "    X_array        = (n_tiles, 300,300,10)\n",
    "    X_processed    = list of (n_valid_pixels, 13)\n",
    "    valid_masks    = list of (300,300)\n",
    "    Y_tiles        = list of (300,300)\n",
    "\n",
    "Output: pixel-level RandomForest model.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1. Build pixel-level dataset\n",
    "# ============================================================\n",
    "\n",
    "def build_pixel_dataset(x_processed_list, y_tiles, valid_masks):\n",
    "    \"\"\"\n",
    "    Convert tiles into supervised pixel-level dataset.\n",
    "\n",
    "    Args:\n",
    "        x_processed_list (list): list of arrays (n_valid_pixels, 13)\n",
    "        y_tiles (list): list of (300,300) label tiles\n",
    "        valid_masks (list): list of boolean masks (300,300)\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            x_pixels (np.ndarray): (N, 13)\n",
    "            y_pixels (np.ndarray): (N,)\n",
    "    \"\"\"\n",
    "    x_pixels = []\n",
    "    y_pixels = []\n",
    "\n",
    "    for x_tile, y_tile, mask in zip(x_processed_list, y_tiles, valid_masks):\n",
    "        y_valid = y_tile[mask]\n",
    "        x_pixels.append(x_tile)\n",
    "        y_pixels.append(y_valid)\n",
    "\n",
    "    x_pixels = np.vstack(x_pixels)\n",
    "    y_pixels = np.hstack(y_pixels)\n",
    "\n",
    "    return x_pixels, y_pixels\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2. Stratified split\n",
    "# ============================================================\n",
    "\n",
    "def split_data(x_data, y_data, test_size=0.2, seed=42):\n",
    "    \"\"\"\n",
    "    Stratified train/test split.\n",
    "\n",
    "    Args:\n",
    "        x_data (np.ndarray): feature matrix\n",
    "        y_data (np.ndarray): labels\n",
    "        test_size (float): test proportion\n",
    "        seed (int): random seed\n",
    "\n",
    "    Returns:\n",
    "        tuple: x_train, x_test, y_train, y_test\n",
    "    \"\"\"\n",
    "    return train_test_split(\n",
    "        x_data,\n",
    "        y_data,\n",
    "        test_size=test_size,\n",
    "        random_state=seed,\n",
    "        stratify=y_data,\n",
    "    )\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3. Train Random Forest\n",
    "# ============================================================\n",
    "\n",
    "def train_random_forest(x_train, y_train):\n",
    "    \"\"\"\n",
    "    Train baseline RandomForest model.\n",
    "\n",
    "    Args:\n",
    "        x_train (np.ndarray): training features\n",
    "        y_train (np.ndarray): training labels\n",
    "\n",
    "    Returns:\n",
    "        RandomForestClassifier: trained model\n",
    "    \"\"\"\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=400,\n",
    "        max_depth=None,\n",
    "        min_samples_split=4,\n",
    "        min_samples_leaf=2,\n",
    "        bootstrap=True,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    model.fit(x_train, y_train)\n",
    "    return model\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4. Evaluation metrics\n",
    "# ============================================================\n",
    "\n",
    "def evaluate(model, x_test, y_test):\n",
    "    \"\"\"\n",
    "    Compute accuracy, F1-score and detailed classification report.\n",
    "\n",
    "    Args:\n",
    "        model: trained model\n",
    "        x_test (np.ndarray): test features\n",
    "        y_test (np.ndarray): test labels\n",
    "\n",
    "    Returns:\n",
    "        tuple: accuracy, f1_score, classification_report (str)\n",
    "    \"\"\"\n",
    "    y_pred = model.predict(x_test)\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "\n",
    "    print(\"\\n--- Evaluation ---\")\n",
    "    print(\"Accuracy:\", acc)\n",
    "    print(\"F1-score:\", f1)\n",
    "    print(report)\n",
    "\n",
    "    return acc, f1, report\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5. Save / Load model\n",
    "# ============================================================\n",
    "\n",
    "def save_model(model, path=\"model_random_forest.joblib\"):\n",
    "    \"\"\"\n",
    "    Save trained model to disk.\n",
    "\n",
    "    Args:\n",
    "        model: trained model\n",
    "        path (str): file path\n",
    "    \"\"\"\n",
    "    directory = os.path.dirname(path)\n",
    "    if directory:\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "    joblib.dump(model, path)\n",
    "    print(f\"✔ Model saved to: {path}\")\n",
    "\n",
    "\n",
    "def load_model(path=\"model_random_forest.joblib\"):\n",
    "    \"\"\"\n",
    "    Load model from disk.\n",
    "\n",
    "    Args:\n",
    "        path (str): model file path\n",
    "\n",
    "    Returns:\n",
    "        model: loaded model\n",
    "    \"\"\"\n",
    "    model = joblib.load(path)\n",
    "    print(f\"✔ Model loaded from: {path}\")\n",
    "    return model\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 6. Predict on a full tile\n",
    "# ============================================================\n",
    "\n",
    "def predict_tile(model, x_processed, valid_mask):\n",
    "    \"\"\"\n",
    "    Predict classes for every valid pixel in a tile.\n",
    "\n",
    "    Args:\n",
    "        model: trained classifier\n",
    "        x_processed (np.ndarray): (n_valid_pixels, 13)\n",
    "        valid_mask (np.ndarray): (300,300) boolean mask\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: (300,300) map with values {-1,0,1}\n",
    "    \"\"\"\n",
    "    preds = model.predict(x_processed)\n",
    "\n",
    "    output = np.full(valid_mask.shape, -1, dtype=np.int8)\n",
    "    output[valid_mask] = preds\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b14730",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "urban_watch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
